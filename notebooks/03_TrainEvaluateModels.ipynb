{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import breizhcrops\n",
    "\n",
    "from breizhcrops.models.LongShortTermMemory import LSTM\n",
    "from breizhcrops.models.TransformerEncoder import TransformerEncoder\n",
    "from breizhcrops.models.TempCNN import TempCNN\n",
    "from breizhcrops.models.MSResNet import MSResNet\n",
    "\n",
    "from breizhcrops import BreizhCrops\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BreizhCrops Pytorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BreizhCrops region frh01\n",
      "Initializing BreizhCrops region frh04\n"
     ]
    }
   ],
   "source": [
    "root = \"../data\"\n",
    "\n",
    "padded_value = -1\n",
    "sequencelength = 45\n",
    "\n",
    "bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "   'B8A', 'B9', 'QA10', 'QA20', 'QA60', 'doa']\n",
    "\n",
    "selected_bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9']\n",
    "\n",
    "selected_band_idxs = np.array([bands.index(b) for b in selected_bands])\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def transform(x):\n",
    "    x = x[x[:, 0] != padded_value, :] # remove padded values\n",
    "\n",
    "    # choose selected bands\n",
    "    x = x[:,selected_band_idxs] * 1e-4 # scale reflectances to 0-1\n",
    "\n",
    "    # choose with replacement if sequencelength smaller als choose_t\n",
    "    replace = False if x.shape[0] >= sequencelength else True\n",
    "    idxs = np.random.choice(x.shape[0], sequencelength, replace=replace)\n",
    "    idxs.sort()\n",
    "\n",
    "    x = x[idxs]\n",
    "\n",
    "    return torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "def target_transform(y):\n",
    "    y = frh01.mapping.loc[y].id\n",
    "    return torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "frh01 = BreizhCrops(root=root, region=\"frh01\", transform=transform, target_transform=transform, padding_value=padded_value)\n",
    "frh04 = BreizhCrops(root=root, region=\"frh04\", transform=transform, target_transform=transform, padding_value=padded_value)\n",
    "#frh02 = BreizhCrops(root=root, region=\"frh02\", transform=transform, target_transform=transform, padded_value=padded_value)\n",
    "#frh03 = BreizhCrops(root=root, region=\"frh03\", transform=transform, target_transform=transform, padded_value=padded_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BreizhCrops region frh04\n",
      "Initializing BreizhCrops region frh01\n",
      "Initializing BreizhCrops region frh02\n",
      "Initializing BreizhCrops region frh03\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-25b178ce486b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mfrh01frh02\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrh01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrh02\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtraindataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrh01frh02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mvaldataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrh03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtestdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrh04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "padded_value = -1\n",
    "sequencelength = 45\n",
    "\n",
    "bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "   'B8A', 'B9', 'QA10', 'QA20', 'QA60', 'doa']\n",
    "\n",
    "selected_bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9']\n",
    "\n",
    "selected_band_idxs = np.array([bands.index(b) for b in selected_bands])\n",
    "\n",
    "def transform(x):\n",
    "    x = x[x[:, 0] != padded_value, :] # remove padded values\n",
    "\n",
    "    # choose selected bands\n",
    "    x = x[:,selected_band_idxs] * 1e-4 # scale reflectances to 0-1\n",
    "\n",
    "    # choose with replacement if sequencelength smaller als choose_t\n",
    "    replace = False if x.shape[0] >= sequencelength else True\n",
    "    idxs = np.random.choice(x.shape[0], sequencelength, replace=replace)\n",
    "    idxs.sort()\n",
    "\n",
    "    x = x[idxs]\n",
    "\n",
    "    return torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "def target_transform(y):\n",
    "    y = frh01.mapping.loc[y].id\n",
    "    return torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n",
    "datapath = \"../data\" # \"/data2/Breizhcrops\"\n",
    "\n",
    "frh04 = breizhcrops.BreizhCrops(region=\"frh04\", root=datapath, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "frh01 = breizhcrops.BreizhCrops(region=\"frh01\", root=datapath, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "frh02 = breizhcrops.BreizhCrops(region=\"frh02\", root=datapath, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "frh03 = breizhcrops.BreizhCrops(region=\"frh03\", root=datapath, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "frh01frh02 = torch.utils.data.ConcatDataset([frh01,frh02])\n",
    "traindataloader = DataLoader(frh01frh02, batch_size=256, shuffle=True, num_workers=0)\n",
    "valdataloader = DataLoader(frh03, batch_size=256, shuffle=False, num_workers=0)\n",
    "testdataloader = DataLoader(frh04, batch_size=256, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMetric:\n",
    "    def __init__(self):\n",
    "        self.values = list()\n",
    "    \n",
    "    def add(self, new):\n",
    "        self.values.append(new)\n",
    "    \n",
    "    def get(self):\n",
    "        return np.array(self.values).mean()\n",
    "    \n",
    "def train(model, optimizer, dataloader, epochs):\n",
    "    model.train()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_log = AverageMetric()\n",
    "\n",
    "        for iteration, data in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = data\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            logprobabilities = model.forward(inputs)\n",
    "\n",
    "            loss = torch.nn.functional.nll_loss(logprobabilities, targets)\n",
    "            loss_log.add(loss.cpu().detach().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch {}: loss {:.2f}\".format(epoch,loss_log.get()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(input_dim=13, hidden_dims=128, num_classes=13, num_layers=3, dropout=0.2, bidirectional=True,\n",
    "                 use_layernorm=True)\n",
    "\n",
    "epochs=5\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, lstm.parameters()),\n",
    "        betas=(0.9, 0.98), eps=1e-09)\n",
    "\n",
    "lstm = train(lstm, optimizer, traindataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.30\n",
      "Epoch 1: loss 0.96\n",
      "Epoch 2: loss 0.89\n",
      "Epoch 3: loss 0.85\n",
      "Epoch 4: loss 0.83\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerEncoder(in_channels=13, len_max_seq=50,\n",
    "    d_word_vec=128, d_model=128, d_inner=512,\n",
    "    n_layers=4, n_head=4, d_k=32, d_v=32,\n",
    "    dropout=0.2, nclasses=13)\n",
    "\n",
    "workers=1\n",
    "epochs=5\n",
    "warmup_steps = 500\n",
    "\n",
    "optimizer = models.transformer.Optim.ScheduledOptim(\n",
    "    torch.optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, transformer.parameters()),\n",
    "        betas=(0.9, 0.98), eps=1e-09),\n",
    "    transformer.d_model, warmup_steps)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=frh01, batch_size=64, num_workers=4)\n",
    "\n",
    "transformer = train(transformer, optimizer, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the TempCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.41\n",
      "Epoch 1: loss 1.19\n",
      "Epoch 2: loss 1.10\n",
      "Epoch 3: loss 1.06\n",
      "Epoch 4: loss 1.03\n"
     ]
    }
   ],
   "source": [
    "tempcnn = TempCNN(input_dim=13, nclasses=13, sequence_length=45, kernel_size=5,hidden_dims=64,dropout=0.5)\n",
    "\n",
    "workers=1\n",
    "epochs=5\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, tempcnn.parameters()),\n",
    "        betas=(0.9, 0.98), lr=1e-3, eps=1e-09, weight_decay=1e-6)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=frh01, batch_size=64, num_workers=4)\n",
    "\n",
    "tempcnn = train(tempcnn, optimizer, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the MSresnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.03\n",
      "Epoch 1: loss 0.86\n",
      "Epoch 2: loss 0.81\n",
      "Epoch 3: loss 0.78\n",
      "Epoch 4: loss 0.75\n"
     ]
    }
   ],
   "source": [
    "msresnet = MSResNet(input_channel=13, num_classes=13, hidden_dims=32)\n",
    "\n",
    "workers=1\n",
    "epochs=5\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, msresnet.parameters()),\n",
    "        betas=(0.9, 0.98), lr=1e-3, eps=1e-09, weight_decay=1e-5)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=frh01, batch_size=64, num_workers=4)\n",
    "\n",
    "msresnet = train(msresnet, optimizer, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained models\n",
    "\n",
    "requires downloaded model files\n",
    "```\n",
    "cd models\n",
    "bash download.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from models/BreizhCrops_rnn/model.pth\n",
      "loading model from models/BreizhCrops_transformer/model.pth\n"
     ]
    }
   ],
   "source": [
    "trainedlstm = LSTM(input_dim=13, hidden_dims=128, nclasses=13, num_rnn_layers=3, dropout=0.2, bidirectional=True,\n",
    "                 use_batchnorm=False, use_layernorm=True)\n",
    "\n",
    "trainedlstm.load(\"models/BreizhCrops_rnn/model.pth\")\n",
    "\n",
    "trainedtransformer = TransformerEncoder(in_channels=13, len_max_seq=407,\n",
    "    d_word_vec=128, d_model=128, d_inner=512,\n",
    "    n_layers=4, n_head=4, d_k=32, d_v=32,\n",
    "    dropout=0.2, nclasses=13)\n",
    "\n",
    "trainedtransformer.load(\"models/BreizhCrops_transformer/model.pth\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainedlstm = trainedlstm.cuda()\n",
    "    trainedtransformer = trainedtransformer.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    logprobabilities = list()\n",
    "    targets_list = list()\n",
    "    inputs_list = list()\n",
    "\n",
    "    for iteration, data in tqdm(enumerate(dataloader)):\n",
    "\n",
    "        inputs, targets = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "        inputs_list.append(inputs.cpu().detach().numpy())\n",
    "        targets_list.append(targets[:,0].cpu().detach().numpy())\n",
    "        logprobabilities.append(model.forward(inputs.transpose(1,2)).cpu().detach().numpy())\n",
    "        \n",
    "    return np.vstack(logprobabilities), np.vstack(inputs_list), np.concatenate(targets_list) # np.vstack(targets_list)\n",
    "\n",
    "\n",
    "def confusion_matrix_to_accuraccies(confusion_matrix):\n",
    "\n",
    "    confusion_matrix = confusion_matrix.astype(float)\n",
    "    # sum(0) <- predicted sum(1) ground truth\n",
    "\n",
    "    total = np.sum(confusion_matrix)\n",
    "    n_classes, _ = confusion_matrix.shape\n",
    "    overall_accuracy = np.sum(np.diag(confusion_matrix)) / total\n",
    "\n",
    "    # calculate Cohen Kappa (https://en.wikipedia.org/wiki/Cohen%27s_kappa)\n",
    "    N = total\n",
    "    p0 = np.sum(np.diag(confusion_matrix)) / N\n",
    "    pc = np.sum(np.sum(confusion_matrix, axis=0) * np.sum(confusion_matrix, axis=1)) / N ** 2\n",
    "    kappa = (p0 - pc) / (1 - pc)\n",
    "\n",
    "    recall = np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=1) + 1e-12)\n",
    "    precision = np.diag(confusion_matrix) / (np.sum(confusion_matrix, axis=0) + 1e-12)\n",
    "    f1 = (2 * precision * recall) / ((precision + recall) + 1e-12)\n",
    "\n",
    "    # Per class accuracy\n",
    "    cl_acc = np.diag(confusion_matrix) / (confusion_matrix.sum(1) + 1e-12)\n",
    "\n",
    "    return overall_accuracy, kappa, precision, recall, f1, cl_acc\n",
    "\n",
    "def build_confusion_matrix(targets, predictions):\n",
    "    \n",
    "    nclasses = len(np.unique(targets))\n",
    "    cm, _, _ = np.histogram2d(targets, predictions, bins=nclasses)\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def print_report(overall_accuracy, kappa, precision, recall, f1, cl_acc):\n",
    "    \n",
    "    report=\"\"\"\n",
    "    overall accuracy: \\t{:.2f}\n",
    "    kappa \\t\\t{:.2f}\n",
    "    precision \\t\\t{:.2f}\n",
    "    recall \\t\\t{:.2f}\n",
    "    f1 \\t\\t\\t{:.2f}\n",
    "    \"\"\".format(overall_accuracy, kappa, precision.mean(), recall.mean(), f1.mean())\n",
    "\n",
    "    print(report)\n",
    "    \n",
    "def evaluate(model, dataset,batchsize=32, workers=4):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batchsize, num_workers=workers)\n",
    "\n",
    "    logprobabilites, inputs, targets = test(model, dataloader)\n",
    "    predictions = logprobabilites.argmax(1)\n",
    "    \n",
    "    confusion_matrix = build_confusion_matrix(targets, predictions)\n",
    "    print_report(*confusion_matrix_to_accuraccies(confusion_matrix))\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on FRH04 region\n",
    "compare pre-trained LSTM/Transformer model with Table 1\n",
    "\n",
    "![title](doc/table1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3937it [00:25, 151.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    overall accuracy: \t0.57\n",
      "    kappa \t\t0.48\n",
      "    precision \t\t0.43\n",
      "    recall \t\t0.39\n",
      "    f1 \t\t\t0.37\n",
      "    \n",
      "Pre-trained LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3937it [00:25, 154.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    overall accuracy: \t0.68\n",
      "    kappa \t\t0.62\n",
      "    precision \t\t0.63\n",
      "    recall \t\t0.58\n",
      "    f1 \t\t\t0.59\n",
      "    \n",
      "This Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3937it [00:28, 137.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    overall accuracy: \t0.62\n",
      "    kappa \t\t0.53\n",
      "    precision \t\t0.51\n",
      "    recall \t\t0.48\n",
      "    f1 \t\t\t0.45\n",
      "    \n",
      "Pre-trained Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3937it [00:29, 134.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    overall accuracy: \t0.69\n",
      "    kappa \t\t0.63\n",
      "    precision \t\t0.60\n",
      "    recall \t\t0.56\n",
      "    f1 \t\t\t0.57\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\"This LSTM\")\n",
    "cm = evaluate(lstm, frh04)\n",
    "\n",
    "print(\"Pre-trained LSTM\")\n",
    "cm = evaluate(trainedlstm, frh04)\n",
    "\n",
    "print(\"This Transformer\")\n",
    "cm = evaluate(transformer, frh04)\n",
    "\n",
    "print(\"Pre-trained Transformer\")\n",
    "cm = evaluate(trainedtransformer, frh04)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
